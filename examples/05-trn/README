05 - NN Training Example

A neural network stratified training job for Cu-Ag using a small Cu-Ag dataset. 
Previously trained Cu.dat and Ag.dat elemental models are provided in NNET/ 

NOTE: Due to the large amount of parsed files, the PARS directory has been compressed, and must
be uncompressed before use.

An example trained output directory is available in ref/

===== REQUIRED =====
-> maise
-> setup
-> PARS/ (directory containing parsed structures for training/testing

===== REQUIRED SETUP CONFIG =====

JOBT  41                training (41) stratified (40) full
NPAR  8                 number of cores for parallel NN training or cell simulation
MINT  0                 gsl minimier type (0) BFGS2 (1) CG-FR (2) CG-PR (3) steepest descent 
MITR  5                 maximum N for NN training or cell opimization steps
NSPC  2                 number of species types
TSPC  29 47             species types (atomic number)
NSYM  30                number of Behler-Parrinello symmetry functions
NCMP  82                total number of NN inputs
TEFS  1                 train NN for (0) E (1) EF
LREG  1e-6              regularization parameter
NTRN -10                number of structures for training (negative means percentage)
NTST -10                number of structures for testing  (negative means percentage)
NNNN  2                 number of hidden layers in MLP
NNNU  10 10             number of neurons in hidden layers in MLP
NNGT  1  1              activation function type for hidden layers
NDIM  3                 (3) crystal (2) film (0) particle
SEED  5                 starting seed for the random number generator - 0 uses time as seed
DATA  ./PARS            location of parsed data
OTPT  ./NNET            directory for storing model parameters and outputs

===== INPUT COMMAND =====
maise 

===== EXPECTED OUTPUT =====

Loading list of parsed data from ./PARS/index.dat

Total number of parameters: 1902
BFGS2 relaxation: 1902 adjustable parameters

     1 0.7745037274234240     0.742860     1.363527     0.837612     1.696877
     2 0.7405085821134600     0.706961     1.371234     0.811836     1.705947
     3 0.3517982997716098     0.299435     1.149101     0.311301     1.421386
     4 0.2677398360798436     0.209578     1.036807     0.154850     1.276348
     5 0.2557858668009388     0.199173     0.998667     0.142432     1.226403
     6 0.2483884887747195     0.193523     0.968935     0.159637     1.187199
     7 0.2120569777371461     0.183153     0.665069     0.124368     0.764066
     8 0.1964475356522932     0.166951     0.644243     0.127379     0.718911
     9 0.1880549542357005     0.156086     0.652689     0.105012     0.709558
    10 0.1875305853668003     0.155352     0.653629     0.110834     0.711297
    11 0.1860939060070139     0.153815     0.651802     0.108810     0.707331
    12 0.1809463425440399     0.151507     0.615604     0.130120     0.666569
    13 0.1676495746705736     0.144123     0.532930     0.143825     0.577264
    14 0.1486089828350177     0.125837     0.491928     0.119855     0.536033
    15 0.1401857218695421     0.117221     0.478423     0.119347     0.531077
    16 0.1374999025907677     0.115024     0.468789     0.120264     0.521297
    17 0.1331546279997594     0.111281     0.455001     0.127115     0.508178
    18 0.1293241472953530     0.110492     0.418170     0.131529     0.460518
    19 0.1261613018227423     0.109880     0.385748     0.136042     0.420983
    20 0.1197830249170868     0.107705     0.326166     0.141026     0.351471
    21 0.1184347263069217     0.105978     0.328994     0.139822     0.355214
    22 0.1172959868592240     0.104648     0.329682     0.136957     0.356394
    23 0.1103193289264113     0.093841     0.360911     0.118227     0.399569
    24 0.1096144607900405     0.093294     0.358081     0.117667     0.395846
    25 0.1052230379280481     0.090628     0.332678     0.114853     0.364145
    26 0.1010358415493823     0.087939     0.309559     0.107119     0.337754
    27 0.0926100935430835     0.081726     0.271052     0.089160     0.294839
    28 0.0784012758511560     0.065794     0.265306     0.066932     0.288491
    29 0.0762955524163080     0.063021     0.267583     0.062666     0.291253
    30 0.0691960144140315     0.057665     0.237987     0.058293     0.257446
    31 0.0667943540433530     0.056022     0.226319     0.057820     0.242897
    32 0.0578620898968203     0.049361     0.187851     0.052302     0.192744
    33 0.0564290876063204     0.048242     0.182143     0.052030     0.184806
    34 0.0555941279017279     0.047801     0.176616     0.050373     0.176730
    35 0.0542586097238701     0.046887     0.169893     0.050172     0.166815
    36 0.0529774977955522     0.046460     0.158388     0.047118     0.150619
    37 0.0502297008599157     0.045092     0.137672     0.044175     0.122074
    38 0.0460484455848915     0.041142     0.128672     0.031923     0.115440
    39 0.0451844749910807     0.040071     0.129894     0.033258     0.118834
    40 0.0425533355110253     0.035845     0.142675     0.028065     0.141016
    41 0.0424191010493749     0.035658     0.142942     0.028656     0.141402
    42 0.0387625493505224     0.032476     0.131657     0.027183     0.125259
    43 0.0379061391268023     0.031580     0.130436     0.025297     0.123791
    44 0.0359767085037607     0.029834     0.125078     0.022195     0.117590
    45 0.0343532917737134     0.028933     0.115210     0.025105     0.102524

===== OUTPUT FILES =====
NNET/model -> trained model file
NNET/err-ene.dat -> energy error of each structure
NNET/err-frc.dat -> force error of each structure
NNET/err-out.dat -> BFGS/CG optimization of NN parameters at each step

For comparison, the corresponding reference output files are provided in the ref/ directory. 

