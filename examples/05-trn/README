05 - NN Training Example

A neural network stratified training job for Cu-Ag using a small Cu-Ag dataset. 
Previously trained Cu.dat and Ag.dat elemental models are provided in NNET/ 

NOTE: Due to the large amount of parsed files, the PARS directory has been compressed, and must
be uncompressed before use.

An example trained output directory is available in ref/

You need to copy the maise executable to the working directory of this example before use.

===== REQUIRED FILES =====
-> maise
-> setup
-> NNET/Ag.dat (elemental model file for silver)
-> NNET/Cu.dat (elemental model file for copper)
-> PARS/       (directory containing parsed structures for training/testing)

===== REQUIRED SETUP CONFIG =====

JOBT  41                training (41) stratified (40) full
NPAR  16                 number of cores for parallel NN training or cell simulation
MINT  0                 gsl minimier type (0) BFGS2 (1) CG-FR (2) CG-PR (3) steepest descent 
MITR  5                 maximum N for NN training or cell opimization steps
NSPC  2                 number of species types
TSPC  29 47             species types (atomic number)
NSYM  30                number of Behler-Parrinello symmetry functions
NCMP  82                total number of NN inputs
TEFS  1                 train NN for (0) E (1) EF
LREG  1e-6              regularization parameter
NTRN  -90               number of structures for training (negative means percentage)
NTST  -10               number of structures for testing  (negative means percentage)
NNNN  2                 number of hidden layers in MLP
NNNU  10 10             number of neurons in hidden layers in MLP
NNGT  1  1              activation function type for hidden layers
SEED  5                 starting seed for the random number generator - 0 uses time as seed
DATA  ./PARS            location of parsed data
OTPT  ./NNET            directory for storing model parameters and outputs

===== EXECUTION COMMAND =====
maise 

===== EXPECTED OUTPUT =====
=======================================================================
|               Module for Ab Initio Structure Evolution              |
|                        version     maise.2.7.00                     |
|                 https://github.com/maise-guide/maise                |
=======================================================================
|                            Model training                           |
=======================================================================

Loading list of parsed data from ./PARS/index.dat

Total number of parameters: 1902
BFGS2 relaxation: 1040 adjustable parameters

     1 0.6907776632474678     0.653798     1.387557     0.674746     1.720906
     2 0.4557599693937525     0.353389     1.790940     0.340026     2.232464
     3 0.4492327175064808     0.345973     1.783132     0.328392     2.223404
     4 0.3661798744309546     0.327591     1.018168     0.320376     1.268725
     5 0.3650227407495643     0.327696     1.000612     0.317110     1.246385
     6 0.3072496005341638     0.299786     0.418857     0.301026     0.501294
     7 0.2998137567811774     0.291950     0.424494     0.271215     0.506528
     8 0.2220123207865006     0.179702     0.811259     0.193551     0.970759
     9 0.1851073783113736     0.133615     0.797177     0.126788     0.940015
    10 0.1815586245983895     0.128427     0.798590     0.121983     0.939571
    11 0.1755187822031165     0.119726     0.798654     0.111374     0.924298
    12 0.1750668060526825     0.119755     0.794635     0.109115     0.919732
    13 0.1693929343242531     0.118185     0.755131     0.089162     0.872840
    14 0.1616075072914776     0.121830     0.660730     0.091869     0.761812
    15 0.1554739364348822     0.120809     0.608974     0.091824     0.699349
    16 0.1511939269597685     0.117610     0.591233     0.091026     0.675602
    17 0.1506453725363850     0.116788     0.592124     0.092034     0.676436
    18 0.1482963502461214     0.115181     0.581251     0.088904     0.664319
    19 0.1481724877286972     0.115065     0.580920     0.089753     0.663911
    20 0.1378013553729158     0.111992     0.499643     0.088344     0.576541
    21 0.1363767076689538     0.110771     0.495019     0.081830     0.571522
    22 0.1350302178984681     0.109070     0.495356     0.086501     0.571689
    23 0.1241222200897913     0.094633     0.499794     0.075693     0.570936
    24 0.1223233762779003     0.092898     0.495198     0.068567     0.565925
    25 0.1204879931654416     0.090924     0.491950     0.065300     0.562207
    26 0.1130437885114483     0.079168     0.502124     0.063062     0.575005
    27 0.1124719051709888     0.078233     0.502826     0.062295     0.576106
    28 0.1100538640201012     0.075333     0.499241     0.056608     0.581979
    29 0.1091781270849718     0.074768     0.495070     0.056508     0.577854
    30 0.1083314908406231     0.074310     0.490516     0.055705     0.573478
    31 0.1012833858911471     0.074080     0.429792     0.050407     0.494484
    32 0.1010444472740820     0.074012     0.428063     0.049386     0.492111
    33 0.0833326707696469     0.067034     0.308054     0.052149     0.340136
    34 0.0826707505013167     0.066507     0.305565     0.053676     0.337316
    35 0.0757486797971910     0.062096     0.269950     0.053640     0.303784
    36 0.0752180416154592     0.061670     0.267974     0.052389     0.301892
    37 0.0720633074961574     0.059014     0.257360     0.051901     0.293218
    38 0.0715979860672132     0.058935     0.252987     0.051948     0.288544
    39 0.0693480475675605     0.058152     0.235110     0.048392     0.271380
    40 0.0687737971369882     0.058079     0.229198     0.048731     0.266127
    41 0.0671949015120464     0.057241     0.219000     0.043057     0.260630
    42 0.0667855356316639     0.056963     0.216948     0.043300     0.260639
    43 0.0665265311800311     0.056627     0.217277     0.041441     0.261189
    44 0.0618806391255734     0.050859     0.219350     0.035591     0.263205
    45 0.0613804586720251     0.050299     0.218910     0.033990     0.260955
    46 0.0606898304667912     0.049367     0.219665     0.032380     0.259822
    47 0.0599060756447895     0.048617     0.217808     0.031558     0.253882
    48 0.0598685036090188     0.048589     0.217646     0.031320     0.253498
    49 0.0575972992617290     0.045814     0.217219     0.027862     0.254034
    50 0.0575030840590062     0.045639     0.217679     0.028305     0.254669
    51 0.0559736062165400     0.042889     0.223808     0.030780     0.261242
    52 0.0555452710596218     0.042165     0.224998     0.031754     0.261276
    53 0.0550279041594057     0.041324     0.226113     0.032606     0.260587
    54 0.0540493285455637     0.040262     0.224391     0.030688     0.257674
    55 0.0540493285455637     0.040262     0.224391     0.030688     0.257674
    56 0.0537838173284364     0.039930     0.224218     0.030455     0.257318
    57 0.0529302247236723     0.039013     0.222593     0.029167     0.256398
    58 0.0518747225804482     0.039335     0.210446     0.025372     0.239241
    59 0.0518351797549267     0.039338     0.210043     0.025489     0.238721
    60 0.0506018205599862     0.039069     0.200110     0.026977     0.222502
    61 0.0505605056971992     0.039074     0.199670     0.026752     0.222200
    62 0.0501769976334332     0.039037     0.196168     0.027473     0.221275
    63 0.0498412004876893     0.038828     0.194458     0.026307     0.218201
    64 0.0498198675207682     0.038815     0.194341     0.026239     0.217931
    65 0.0492079625306436     0.038315     0.192140     0.025724     0.212779
    66 0.0486154854434123     0.037823     0.190063     0.025484     0.209840
    67 0.0436870407815751     0.033154     0.177030     0.022427     0.188363
    68 0.0435119084032715     0.033081     0.175886     0.022381     0.186428
    69 0.0432902042016965     0.033177     0.173041     0.022233     0.181127
    70 0.0431841257155487     0.033218     0.171706     0.022377     0.178668
    71 0.0430029548925384     0.033333     0.169068     0.022371     0.173528
    72 0.0428871781613381     0.033402     0.167388     0.022669     0.170290
    73 0.0427366488678108     0.033508     0.165059     0.022806     0.165667
    74 0.0425909468268525     0.033608     0.162800     0.023268     0.161227
    75 0.0424391050578629     0.033697     0.160536     0.023506     0.156730
    76 0.0422059868001260     0.033822     0.157105     0.024194     0.149891
    77 0.0422059868001260     0.033822     0.157105     0.024194     0.149891
    78 0.0420336857246131     0.033658     0.156682     0.023934     0.149373
    79 0.0400858454144472     0.031658     0.153009     0.022496     0.147219
    80 0.0392999237181038     0.030816     0.151772     0.022508     0.145579
    81 0.0386072961361469     0.030029     0.150990     0.022384     0.144877
    82 0.0374315798473391     0.028708     0.149466     0.021291     0.138964
    83 0.0372880832598530     0.028530     0.149397     0.021073     0.138531
    84 0.0365319756710596     0.027671     0.148424     0.021873     0.135583
    85 0.0364946224517604     0.027632     0.148343     0.021983     0.135502
    86 0.0361000939753681     0.027202     0.147681     0.023094     0.134918
    87 0.0360797872794170     0.027182     0.147638     0.023312     0.134894
    88 0.0357981054138270     0.026814     0.147583     0.023958     0.134498
    89 0.0357981054138270     0.026814     0.147583     0.023958     0.134498
    90 0.0357198692478038     0.026683     0.147770     0.024008     0.134585
    91 0.0356184618856647     0.026493     0.148147     0.023209     0.134720
    92 0.0352072330625890     0.025820     0.148938     0.021007     0.136440
    93 0.0351785242122456     0.025782     0.148931     0.020854     0.136597
    94 0.0348048128712951     0.025186     0.149475     0.020621     0.140702
    95 0.0347817758932623     0.025135     0.149600     0.020531     0.141005
    96 0.0345843650445090     0.024660     0.150887     0.020318     0.143997
    97 0.0345562998831611     0.024585     0.151108     0.020137     0.144288
    98 0.0344443368793060     0.024267     0.152109     0.019739     0.145629
    99 0.0344234860941795     0.024205     0.152309     0.019549     0.145743
   100 0.0343743267077907     0.024048     0.152839     0.019327     0.146125

ERROR ECNT CALLS =    567
ERROR FCNT CALLS =    320
TOT. RESI. ERROR =  0.001182  0.549610 -0.548429 (final,initial,difference)


 job times: real= 1.93   user= 28.85   sys= 0.61   total CPU= 29.46 (sec)

Stand. dev. ENE           0.762006                        343
Train error ENE FRC TOT   0.024048  0.152839  0.018604    309    798
Test  error ENE FRC TOT   0.019327  0.146125  0.013978     34     93

===== OUTPUT FILES =====
NNET/model -> trained model file
NNET/err-ene.dat -> energy error of each structure
NNET/err-frc.dat -> force error of each structure
NNET/err-out.dat -> BFGS/CG optimization of NN parameters at each step

For comparison, the corresponding reference output files are provided in the ref/ directory. 


