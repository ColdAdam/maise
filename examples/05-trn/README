05 - NN Training Example

A neural network stratified training job for Cu-Ag using a small Cu-Ag dataset. 
Previously trained Cu.dat and Ag.dat elemental models are provided in NNET/ 

NOTE: Due to the large amount of parsed files, the PARS directory has been compressed, and must
be uncompressed before use.

An example trained output directory is available in ref/

You need to copy the maise executable to the working directory of this example before use.

===== REQUIRED =====
-> maise
-> setup
-> PARS/ (directory containing parsed structures for training/testing

===== REQUIRED SETUP CONFIG =====

JOBT  41                training (41) stratified (40) full
NPAR  8                 number of cores for parallel NN training or cell simulation
MINT  0                 gsl minimier type (0) BFGS2 (1) CG-FR (2) CG-PR (3) steepest descent 
MITR  5                 maximum N for NN training or cell opimization steps
NSPC  2                 number of species types
TSPC  29 47             species types (atomic number)
NSYM  30                number of Behler-Parrinello symmetry functions
NCMP  82                total number of NN inputs
TEFS  1                 train NN for (0) E (1) EF
LREG  1e-6              regularization parameter
NTRN -10                number of structures for training (negative means percentage)
NTST -10                number of structures for testing  (negative means percentage)
NNNN  2                 number of hidden layers in MLP
NNNU  10 10             number of neurons in hidden layers in MLP
NNGT  1  1              activation function type for hidden layers
NDIM  3                 (3) crystal (2) film (0) particle
SEED  5                 starting seed for the random number generator - 0 uses time as seed
DATA  ./PARS            location of parsed data
OTPT  ./NNET            directory for storing model parameters and outputs

===== INPUT COMMAND =====
maise 

===== EXPECTED OUTPUT =====
=======================================================================
|               Module for Ab Initio Structure Evolution              |
|                        version     maise.2.7.00                     |
|                 https://github.com/maise-guide/maise                |
=======================================================================
|                            Model training                           |
=======================================================================

Loading list of parsed data from ./PARS/index.dat

Total number of parameters: 1902
BFGS2 relaxation: 1902 adjustable parameters

     1 0.7745037274234239     0.742860     1.363527     0.837612     1.696877
     2 0.7405085821134599     0.706961     1.371234     0.811836     1.705947
     3 0.3517982997716284     0.299435     1.149101     0.311301     1.421386
     4 0.2677398360798473     0.209578     1.036807     0.154850     1.276348
     5 0.2557858668009441     0.199173     0.998667     0.142432     1.226403
     6 0.2483884887747255     0.193523     0.968935     0.159637     1.187199
     7 0.2120569777371854     0.183153     0.665069     0.124368     0.764066
     8 0.1964475356523126     0.166951     0.644243     0.127379     0.718911
     9 0.1880549542357042     0.156086     0.652689     0.105012     0.709558
    10 0.1875305853668032     0.155352     0.653629     0.110834     0.711297
    11 0.1860939060070183     0.153815     0.651802     0.108810     0.707331
    12 0.1809463425440579     0.151507     0.615604     0.130120     0.666569
    13 0.1676495746705789     0.144123     0.532930     0.143825     0.577264
    14 0.1486089828349949     0.125837     0.491928     0.119855     0.536033
    15 0.1401857218695557     0.117221     0.478423     0.119347     0.531077
    16 0.1374999025908252     0.115024     0.468789     0.120264     0.521297
    17 0.1331546279997301     0.111281     0.455001     0.127115     0.508178
    18 0.1293241472952300     0.110492     0.418170     0.131529     0.460518
    19 0.1261613018216616     0.109880     0.385748     0.136042     0.420983
    20 0.1197830249165831     0.107705     0.326166     0.141026     0.351471
    21 0.1184347263066483     0.105978     0.328994     0.139822     0.355214
    22 0.1172959868591906     0.104648     0.329682     0.136957     0.356394
    23 0.1103193289263033     0.093841     0.360911     0.118227     0.399569
    24 0.1096144607900383     0.093294     0.358081     0.117667     0.395846
    25 0.1052230379276158     0.090628     0.332678     0.114853     0.364145
    26 0.1010358415499432     0.087939     0.309559     0.107119     0.337754
    27 0.0926100935412303     0.081726     0.271052     0.089160     0.294839
    28 0.0784012758459643     0.065794     0.265306     0.066932     0.288491
    29 0.0762955524129096     0.063021     0.267583     0.062666     0.291253
    30 0.0691960144087353     0.057665     0.237987     0.058293     0.257446
    31 0.0667943540301446     0.056022     0.226319     0.057820     0.242897
    32 0.0578620898786175     0.049361     0.187851     0.052302     0.192744
    33 0.0564290875963897     0.048242     0.182143     0.052030     0.184806
    34 0.0555941278930688     0.047801     0.176616     0.050373     0.176730
    35 0.0542586097122227     0.046887     0.169893     0.050172     0.166815
    36 0.0529774977812777     0.046460     0.158388     0.047118     0.150619
    37 0.0502297008441443     0.045092     0.137672     0.044175     0.122074
    38 0.0460484454976566     0.041142     0.128672     0.031923     0.115440
    39 0.0451844749497714     0.040071     0.129894     0.033258     0.118834
    40 0.0425533354998308     0.035845     0.142675     0.028065     0.141016
    41 0.0424191010344144     0.035658     0.142942     0.028656     0.141402
    42 0.0387625491337631     0.032476     0.131657     0.027183     0.125259
    43 0.0379061389067543     0.031580     0.130436     0.025297     0.123791
    44 0.0359767082442825     0.029834     0.125078     0.022195     0.117590
    45 0.0343532914617759     0.028933     0.115210     0.025105     0.102524
    46 0.0342557782673434     0.028867     0.114728     0.024642     0.101874
    47 0.0325297292775513     0.027187     0.111112     0.027776     0.099182
    48 0.0309006003064472     0.025909     0.104749     0.026789     0.096252
    49 0.0305921209167820     0.025620     0.103991     0.027740     0.096373
    50 0.0254143433404075     0.020787     0.090936     0.022583     0.095111
    51 0.0253655428516156     0.020717     0.091033     0.022694     0.095309
    52 0.0253655428516156     0.020717     0.091033     0.022694     0.095309
    53 0.0253277102743971     0.020681     0.090938     0.022515     0.095145
    54 0.0251557266257476     0.020436     0.091240     0.022469     0.095127
    55 0.0239905219598694     0.019246     0.089078     0.020907     0.095068
    56 0.0236873329051061     0.019048     0.087571     0.020701     0.094062
    57 0.0231240524764753     0.018657     0.084960     0.018627     0.092478
    58 0.0231240524764753     0.018657     0.084960     0.018627     0.092478
    59 0.0230695861056155     0.018580     0.085044     0.018764     0.092658
    60 0.0229105178428698     0.018384     0.085026     0.018070     0.092853
    61 0.0225944893432152     0.018104     0.084077     0.017467     0.094128
    62 0.0225670274529417     0.018084     0.083955     0.017350     0.094206
    63 0.0221338244024269     0.017549     0.083885     0.017801     0.098597
    64 0.0218402512868960     0.017108     0.084429     0.018403     0.101164
    65 0.0218078831793723     0.017062     0.084469     0.018531     0.101470
    66 0.0211967427773819     0.016550     0.082366     0.018777     0.103554
    67 0.0210263072508318     0.016409     0.081763     0.018856     0.104076
    68 0.0207834019403824     0.016253     0.080552     0.018509     0.104264
    69 0.0205408109221924     0.016000     0.080105     0.018189     0.105226
    70 0.0204950653586109     0.015957     0.079983     0.017977     0.105285
    71 0.0200824311602912     0.015224     0.081445     0.017679     0.107784
    72 0.0200371715882298     0.015179     0.081341     0.017613     0.107789
    73 0.0193966477798607     0.014408     0.080760     0.018284     0.109189
    74 0.0189633438542127     0.014217     0.078037     0.018677     0.108203
    75 0.0186623932886371     0.013862     0.077703     0.018857     0.109341
    76 0.0186214944475172     0.013840     0.077470     0.018951     0.109198
    77 0.0186054063195245     0.013817     0.077480     0.018955     0.109252
    78 0.0185238159412216     0.013799     0.076845     0.018831     0.108222
    79 0.0185114292699510     0.013792     0.076780     0.018838     0.108048
    80 0.0185114292699509     0.013792     0.076780     0.018838     0.108048
    81 0.0184997458507241     0.013790     0.076684     0.018860     0.107873
    82 0.0184109127275330     0.013746     0.076160     0.019229     0.106420
    83 0.0183288592779397     0.013705     0.075677     0.019227     0.104962
    84 0.0183014898111800     0.013704     0.075431     0.019250     0.104275
    85 0.0181105143364816     0.013560     0.074651     0.019173     0.100614
    86 0.0177351657311405     0.013220     0.073511     0.018442     0.094609
    87 0.0175608389998608     0.013034     0.073174     0.018104     0.091848
    88 0.0174740491857945     0.012904     0.073266     0.018293     0.090717
    89 0.0174613119214843     0.012878     0.073322     0.018293     0.090656
    90 0.0173388965536225     0.012674     0.073576     0.018153     0.089715
    91 0.0172817930560103     0.012567     0.073767     0.018345     0.089057
    92 0.0172538148288817     0.012503     0.073932     0.018432     0.088851
    93 0.0172258677839858     0.012491     0.073757     0.018256     0.088522
    94 0.0172213539897304     0.012491     0.073715     0.018262     0.088423
    95 0.0171922334362075     0.012464     0.073634     0.018337     0.088030
    96 0.0170056359781847     0.012309     0.072962     0.017978     0.086074
    97 0.0169170401072250     0.012242     0.072594     0.017763     0.085011
    98 0.0167735458885025     0.012093     0.072277     0.017938     0.083466
    99 0.0167543009758930     0.012066     0.072273     0.017959     0.083363
   100 0.0166562229639309     0.011962     0.072068     0.017640     0.082822

ERROR ECNT CALLS =    514
ERROR FCNT CALLS =    306
TOT. RESI. ERROR =  0.000277  7.525728 -7.525450 (final,initial,difference)


 job times: real= 2.01   user= 30.27   sys= 0.24   total CPU= 30.51 (sec)

Stand. dev. ENE           0.762006                        343
Train error ENE FRC TOT   0.011962  0.072068  0.010121    309    798
Test  error ENE FRC TOT   0.017640  0.082822  0.012707     34     93

===== OUTPUT FILES =====
NNET/model -> trained model file
NNET/err-ene.dat -> energy error of each structure
NNET/err-frc.dat -> force error of each structure
NNET/err-out.dat -> BFGS/CG optimization of NN parameters at each step

For comparison, the corresponding reference output files are provided in the ref/ directory. 

